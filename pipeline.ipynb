{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# SRC_PATH = './src'\n",
    "# if SRC_PATH not in sys.path:\n",
    "#     sys.path.insert(-1, SRC_PATH)\n",
    "#     print(sys.path)\n",
    "# from utils import *\n",
    "# from models import *\n",
    "# from data_factory import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8.2\n"
     ]
    }
   ],
   "source": [
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import torch\n",
    "\n",
    "import torchvision\n",
    "print(torchvision.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = './data/cifar-10-batches-py'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load Data\n",
    "1. data -- a 10000x3072 numpy array of uint8s. Each row of the array stores a 32x32 colour image. The first 1024 entries contain the red channel values, the next 1024 the green, and the final 1024 the blue. The image is stored in row-major order, so that the first 32 entries of the array are the red channel values of the first row of the image.\n",
    "\n",
    "2.  labels -- a list of 10000 numbers in the range 0-9. The number at index i indicates the label of the ith image in the array data.\n",
    "\n",
    "3. **batches.meta** It too contains a Python dictionary object. It has the following entries:\n",
    "\n",
    "4. label_names -- a 10-element list which gives meaningful names to the numeric labels in the labels array described above. For example, label_names[0] == \"airplane\", label_names[1] == \"automobile\", etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print(os.listdir(DATA_DIR))\n",
    "# # dict_keys([b'batch_label', b'labels', b'data', b'filenames'])\n",
    "# # dict_keys([b'num_cases_per_batch', b'label_names', b'num_vis'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. visualize a random image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idx = np.random.randint(0, train_data.shape[0])\n",
    "# plt.imshow(train_data[idx])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Prepare Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize = transforms.Compose([\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "# ])\n",
    "\n",
    "# stats = ((0.507, 0.487, 0.441), (0.267, 0.256, 0.276))\n",
    "# train_normalizer = transforms.Compose([transforms.RandomCrop(32, padding=4, padding_mode='reflect'), \n",
    "#                          transforms.RandomHorizontalFlip(), \n",
    "#                          transforms.ToTensor(), \n",
    "#                          transforms.Normalize(*stats,inplace=True)])\n",
    "\n",
    "# train_set = datasets.CIFAR10(root='./data', train=True, download=False, transform=train_normalize)\n",
    "# test_set = datasets.CIFAR10(root='./data', train=False, download=False, transform=normalize)\n",
    "\n",
    "# train_data_loader = DataLoader(train_set, batch_size = 16, shuffle=True, num_workers=0)\n",
    "# test_data_loader = DataLoader(test_set, batch_size = 16, shuffle=False, num_workers=0)\n",
    "\n",
    "# train_val_set = datasets.CIFAR10(root='./data', train=True, download=False, transform=train_normalizer)\n",
    "# train_set, val_set = random_split(train_val_set, [len(train_val_set) -  int(0.2 * 50000),  int(0.2 * 50000)])\n",
    "\n",
    "# data_generator = CIFAR10DataPrep()\n",
    "# data_generator.prepare_dataloader()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Load Pretrained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class ResNet50(nn.Module):\n",
    "    \n",
    "#     def __init__(self, model_config):\n",
    "#         super().__init__()\n",
    "#         resnet = models.resnet50(pretrained=True)\n",
    "#         modules = list(resnet.children())[:-1] \n",
    "        \n",
    "#         self.num_classes = model_config['num_classes']\n",
    "#         self.resnet = nn.Sequential(*modules)\n",
    "#         self.fc = nn.Linear(resnet.fc.in_features, self.num_classes)\n",
    "# #         self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "#     def forward(self, x):\n",
    "# #         print(x.shape)\n",
    "#         features = self.resnet(x).view(-1, 2048)\n",
    "# #         print(features.shape)\n",
    "#         out = self.fc(features)\n",
    "# #         out = self.softmax(out)\n",
    "#         return out\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "model_config = {}\n",
    "model_config['num_classes'] = 10\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "model = ResNet50(model_config).to(device)\n",
    "    \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(2):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, batch in enumerate(train_data_loader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = batch[0].to(device), batch[1].to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 10 == 1:    # print every 2000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 1000))\n",
    "            running_loss = 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Testing\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in test_data_loader:\n",
    "        images, labels = data\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
    "    100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# cp1 = torch.load('./checkpoint/ckpt1.pth')\n",
    "# cp2 = torch.load('./checkpoint/ckpt2.pth')\n",
    "\n",
    "# cp1t = torch.load('./checkpoint/testing1.pth')\n",
    "# cp2t = torch.load('./checkpoint/testing2.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(cp1['acc'], cp1['epoch'], cp1t['acc'])\n",
    "# print(cp2['acc'], cp2['epoch'], cp2t['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
